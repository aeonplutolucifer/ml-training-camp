{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-camp-homework02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODM1BuX8VLPde83NiGfmGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeonplutolucifer/ml-training-camp/blob/main/ML_camp_homework02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOLULxTqWIQY",
        "outputId": "0f1b4dcf-d220-4f98-a892-33d80a538f6d"
      },
      "source": [
        "!pip install lightgbm xgboost catboost category-encoders sklearn pandas==1.1.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/41/24e14322b9986cf72a8763e0a0a69cc256cf963cf9502c8f0044a62c1ae8/catboost-0.26-cp37-none-manylinux1_x86_64.whl (69.2MB)\n",
            "\u001b[K     |████████████████████████████████| 69.2MB 42kB/s \n",
            "\u001b[?25hCollecting category-encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost, category-encoders\n",
            "Successfully installed catboost-0.26 category-encoders-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD54c8iUXB_B",
        "outputId": "847dea1e-40ce-4ca3-f553-945a4e11656b"
      },
      "source": [
        "!git clone https://github.com/aeonplutolucifer/ml-training-camp-material.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-training-camp-material'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 1), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6KwaLsZJpR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "x_train = pd.read_csv('./ml-training-camp-material/final/train_final.csv', engine='python')\n",
        "x_test = pd.read_csv('./ml-training-camp-material/final/test_final.csv', engine='python')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJnMFvlUbUWF",
        "outputId": "ee1a2e00-a725-4d7d-d0a1-7bcf927e0ff2"
      },
      "source": [
        "TRAIN_IDX=x_train.shape[0]\n",
        "TEST_IDX = TRAIN_IDX + x_test.shape[0]\n",
        "print (TRAIN_IDX)\n",
        "print (TEST_IDX)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "286mdeF6beFt"
      },
      "source": [
        "data = pd.concat([x_train, x_test], axis=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_3jX-w1bzcX"
      },
      "source": [
        "data.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW9HnW5sfxsV",
        "outputId": "0bfa5191-f65e-49c4-e108-2f351706c877"
      },
      "source": [
        "print (data.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjORNjDicJvv"
      },
      "source": [
        "train = data.iloc[:TRAIN_IDX, :]\n",
        "test = data.iloc[TRAIN_IDX:TEST_IDX, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fPmhsvicQFA"
      },
      "source": [
        "import lightgbm as lgb\n",
        "train_dataset = lgb.Dataset(train.drop(columns='loan_status'), train['loan_status'])\n",
        "test_dataset = lgb.Dataset(test.drop(columns='loan_status'), test['loan_status'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYRY5QBWixwi"
      },
      "source": [
        "import io\n",
        "import multiprocessing\n",
        "from contextlib import redirect_stdout\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass, asdict\n",
        "import hyperopt.pyll\n",
        "from hyperopt import fmin, tpe, hp\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "\n",
        "import copy\n",
        "cpu_count = 4\n",
        "use_gpu = False\n",
        "@dataclass\n",
        "class LGBOpt:\n",
        "    num_threads: any = hp.choice('num_threads', [cpu_count])\n",
        "    num_leaves: any = hp.choice('num_leaves', [64])\n",
        "    metric: any = hp.choice('metric', ['binary_error'])\n",
        "    num_round: any = hp.choice('num_rounds', [1000])\n",
        "    objective: any = hp.choice('objective', ['binary'])\n",
        "    learning_rate: any = hp.uniform('learning_rate', 0.01, 0.1)\n",
        "    feature_fraction: any = hp.uniform('feature_fraction', 0.5, 1.0)\n",
        "    bagging_fraction: any = hp.uniform('bagging_fraction', 0.8, 1.0)\n",
        "    device_type: any = hp.choice('device_tpye', ['gpu']) if use_gpu else hp.choice('device_type',\n",
        "                                                                                   ['cpu'])\n",
        "    boosting: any = hp.choice('boosting', ['gbdt', 'dart', 'goss'])\n",
        "    extra_trees: any = hp.choice('extra_tress', [False, True])\n",
        "    drop_rate: any = hp.uniform('drop_rate', 0, 0.2)\n",
        "    uniform_drop: any = hp.choice('uniform_drop', [True, False])\n",
        "    lambda_l1: any = hp.uniform('lambda_l1', 0, 10)  # TODO: Check range\n",
        "    lambda_l2: any = hp.uniform('lambda_l2', 0, 10)  # TODO: Check range\n",
        "    min_gain_to_split: any = hp.uniform('min_gain_to_split', 0, 1)  # TODO: Check range\n",
        "    min_data_in_bin = hp.choice('min_data_in_bin', [3, 5, 10, 15, 20, 50])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_common_params():\n",
        "        return {'num_thread': 4, 'num_leaves': 12, 'metric': 'binary', 'objective': 'binary',\n",
        "                'num_round': 1000, 'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChVCW92iiiL9"
      },
      "source": [
        "class FitterBase(object):\n",
        "    def __init__(self, label, metric, max_eval=100, opt=None):\n",
        "        self.label = label\n",
        "        self.metric = metric\n",
        "        self.opt_params = dict()\n",
        "        self.max_eval = max_eval\n",
        "        self.opt = opt\n",
        "\n",
        "    def get_loss(self, y, y_pred):\n",
        "        if self.metric == 'error':\n",
        "            return 1 - accuracy_score(y, y_pred)\n",
        "        elif self.metric == 'precision':\n",
        "            return 1 - precision_score(y, y_pred)\n",
        "        elif self.metric == 'recall':\n",
        "            return 1 - recall_score(y, y_pred)\n",
        "        elif self.metric == 'macro_f1':\n",
        "            return 1 - f1_score(y, y_pred, average='macro')\n",
        "        elif self.metric == 'micro_f1':\n",
        "            return 1 - f1_score(y, y_pred, average='micro')\n",
        "        elif self.metric == 'auc':  # TODO: Add a warning checking if y_predict is all [0, 1], it should be probability\n",
        "            return 1 - roc_auc_score(y, y_pred)\n",
        "        else:\n",
        "            raise Exception(\"Not implemented yet.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d_8IU7jicrN"
      },
      "source": [
        "class LGBFitter(FitterBase):\n",
        "    def __init__(self, label='label', metric='error', opt: LGBOpt = None, max_eval=100):\n",
        "        super(LGBFitter, self).__init__(label, metric, max_eval)\n",
        "        if opt is not None:\n",
        "            self.opt = opt\n",
        "        else:\n",
        "            self.opt = LGBOpt()\n",
        "        self.best_round = None\n",
        "        self.clf = None\n",
        "\n",
        "    def train(self, train_df, eval_df, params=None, use_best_eval=True):\n",
        "        self.best_round = None\n",
        "        dtrain = lgb.Dataset(train_df.drop(columns=[self.label]), train_df[self.label])\n",
        "        deval = lgb.Dataset(eval_df.drop(columns=[self.label]), eval_df[self.label])\n",
        "        evallist = [dtrain, deval]\n",
        "        if params is None:\n",
        "            use_params = deepcopy(self.opt_params)\n",
        "        else:\n",
        "            use_params = deepcopy(params)\n",
        "\n",
        "        num_round = use_params.pop('num_round')\n",
        "        if use_best_eval:\n",
        "            with io.StringIO() as buf, redirect_stdout(buf):\n",
        "                self.clf = lgb.train(use_params, dtrain, num_round, valid_sets=evallist)\n",
        "                output = buf.getvalue().split(\"\\n\")\n",
        "            min_error = np.inf\n",
        "            min_index = 0\n",
        "            for idx in range(len(output) - 1):\n",
        "                if len(output[idx].split(\"\\t\")) == 3:\n",
        "                    temp = float(output[idx].split(\"\\t\")[2].split(\":\")[1])\n",
        "                    if min_error > temp:\n",
        "                        min_error = temp\n",
        "                        min_index = int(output[idx].split(\"\\t\")[0][1:-1])\n",
        "            print(\"The minimum is attained in round %d\" % (min_index + 1))\n",
        "            self.best_round = min_index + 1\n",
        "            return output\n",
        "        else:\n",
        "            with io.StringIO() as buf, redirect_stdout(buf):\n",
        "                self.clf = lgb.train(use_params, dtrain, num_round, valid_sets=evallist)\n",
        "                output = buf.getvalue().split(\"\\n\")\n",
        "            self.best_round = num_round\n",
        "            return output\n",
        "\n",
        "    def search(self, train_df, eval_df, use_best_eval=True):\n",
        "        self.opt_params = dict()\n",
        "\n",
        "        def train_impl(params):\n",
        "            self.train(train_df, eval_df, params, use_best_eval)\n",
        "            if self.metric == 'auc':\n",
        "                y_pred = self.clf.predict(eval_df.drop(columns=[self.label]), num_iteration=self.best_round)\n",
        "            else:\n",
        "                y_pred = (self.clf.predict(eval_df.drop(columns=[self.label]),\n",
        "                                           num_iteration=self.best_round) > 0.5).astype(int)\n",
        "            return self.get_loss(eval_df[self.label], y_pred)\n",
        "\n",
        "        self.opt_params = fmin(train_impl, asdict(self.opt), algo=tpe.suggest, max_evals=self.max_eval)\n",
        "\n",
        "    def search_k_fold(self, k_fold, data, use_best_eval=True):\n",
        "        self.opt_params = dict()\n",
        "\n",
        "        def train_impl_nfold(params):\n",
        "            loss = list()\n",
        "            for train_id, eval_id in k_fold.split(data):\n",
        "                train_df = data.loc[train_id]\n",
        "                eval_df = data.loc[eval_id]\n",
        "                self.train(train_df, eval_df, params, use_best_eval)\n",
        "                if self.metric == 'auc':\n",
        "                    y_pred = self.clf.predict(eval_df.drop(columns=[self.label]), num_iteration=self.best_round)\n",
        "                else:\n",
        "                    y_pred = (self.clf.predict(eval_df.drop(columns=[self.label]),\n",
        "                                               num_iteration=self.best_round) > 0.5).astype(int)\n",
        "                loss.append(self.get_loss(eval_df[self.label], y_pred))\n",
        "            return np.mean(loss)\n",
        "\n",
        "        self.opt_params = fmin(train_impl_nfold, asdict(self.opt), algo=tpe.suggest, max_evals=self.max_eval)\n",
        "\n",
        "    def train_k_fold(self, k_fold, train_data, test_data, params=None, drop_test_y=True, use_best_eval=True):\n",
        "        acc_result = list()\n",
        "        train_pred = np.empty(train_data.shape[0])\n",
        "        test_pred = np.empty(test_data.shape[0])\n",
        "        if drop_test_y:\n",
        "            dtest = test_data.drop(columns=self.label)\n",
        "        else:\n",
        "            dtest = test_data\n",
        "\n",
        "        models = list()\n",
        "        for train_id, eval_id in k_fold.split(train_data):\n",
        "            train_df = train_data.loc[train_id]\n",
        "            eval_df = train_data.loc[eval_id]\n",
        "            self.train(train_df, eval_df, params, use_best_eval)\n",
        "            models.append(copy.deepcopy(self.clf))\n",
        "            train_pred[eval_id] = self.clf.predict(eval_df.drop(columns=self.label), num_iteration=self.best_round)\n",
        "            if self.metric == 'auc':\n",
        "                y_pred = self.clf.predict(eval_df.drop(columns=[self.label]), num_iteration=self.best_round)\n",
        "            else:\n",
        "                y_pred = (self.clf.predict(eval_df.drop(columns=[self.label]),\n",
        "                                           num_iteration=self.best_round) > 0.5).astype(int)\n",
        "            acc_result.append(self.get_loss(eval_df[self.label], y_pred))\n",
        "            test_pred += self.clf.predict(dtest, num_iteration=self.best_round)\n",
        "        test_pred /= k_fold.n_splits\n",
        "        return train_pred, test_pred, acc_result, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtCi9MhKjIIk"
      },
      "source": [
        "fitter = LGBFitter(label='loan_status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9djjSC7dCye",
        "outputId": "dd320b80-7a22-4d74-ef61-aa7f1c8dfd7b"
      },
      "source": [
        "for num_leaves in [50]:\n",
        "  params = {'num_thread': 4, 'num_leaves': num_leaves, 'metric': 'binary', 'objective': 'binary',\n",
        "                'num_round': 2000, 'learning_rate': 0.008, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "  _,_,error,_ = fitter.train_k_fold(kfold, train, test, params = params)\n",
        "  print (params)\n",
        "  print (1-np.mean(error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum is attained in round 887\n",
            "The minimum is attained in round 599\n",
            "The minimum is attained in round 723\n",
            "The minimum is attained in round 668\n",
            "The minimum is attained in round 614\n",
            "{'num_thread': 4, 'num_leaves': 50, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.008, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
            "0.9187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dynm1umdF18"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojwdige2jTOX",
        "outputId": "ff6797b9-9be2-44c2-c9c2-8049bded13d2"
      },
      "source": [
        "_,_,error,_ = fitter.train_k_fold(kfold, train, test, params = params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum is attained in round 370\n",
            "The minimum is attained in round 243\n",
            "The minimum is attained in round 274\n",
            "The minimum is attained in round 264\n",
            "The minimum is attained in round 253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17_DQvCImTFu",
        "outputId": "e449acae-cbf8-4eac-ec77-4cbec494a34c"
      },
      "source": [
        "import numpy as np\n",
        "print (params)\n",
        "print (1-np.mean(error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'num_thread': 4, 'num_leaves': 50, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
            "0.91866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7hoKXv_pYXW"
      },
      "source": [
        "**Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUi2PO_FpgNV"
      },
      "source": [
        "Baseline:\n",
        "{'num_thread': 4, 'num_leaves': 12, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91978\n",
        "\n",
        "1: increase num_leaves to 50\n",
        "{'num_thread': 4, 'num_leaves': 50, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91866\n",
        "\n",
        "2: create a for loop to test on different num_leaves [28,44,60,76,92,108,124]\n",
        "{'num_thread': 4, 'num_leaves': 28, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91926\n",
        "\n",
        "{'num_thread': 4, 'num_leaves': 44, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91854\n",
        "\n",
        "{'num_thread': 4, 'num_leaves': 60, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91802\n",
        "\n",
        "{'num_thread': 4, 'num_leaves': 76, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.9179\n",
        "\n",
        "Stop on 76 given increse num_leaves will decrease accurancy\n",
        "\n",
        "3: try again on num_leaves 10\n",
        "{'num_thread': 4, 'num_leaves': 10, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.92012\n",
        "\n",
        "4: After monitor the round num ,1000 is enough\n",
        "The minimum is attained in round 530\n",
        "The minimum is attained in round 537\n",
        "The minimum is attained in round 443\n",
        "The minimum is attained in round 620\n",
        "The minimum is attained in round 353\n",
        "{'num_thread': 4, 'num_leaves': 10, 'metric': 'binary', 'objective': 'binary', 'num_round': 1000, 'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.92012\n",
        "\n",
        "5: decrese learning rate to 0.015 and accurancy drop\n",
        "{'num_thread': 4, 'num_leaves': 10, 'metric': 'binary', 'objective': 'binary', 'num_round': 1000, 'learning_rate': 0.015, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91974\n",
        "\n",
        "6: increase the num_leaves again to 12 \n",
        "{'num_thread': 4, 'num_leaves': 12, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91992\n",
        "\n",
        "7：continue decrease learning_rate and increase num_leaves\n",
        "{'num_thread': 4, 'num_leaves': 20, 'metric': 'binary', 'objective': 'binary', 'num_round': 2000, 'learning_rate': 0.008, 'feature_fraction': 0.8, 'bagging_fraction': 0.8}\n",
        "0.91922\n",
        "\n",
        "And found the best result is on 4 : 0.92012"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqKLw6oivyxe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}